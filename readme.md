## Формулировка задачи
Большинство веб-страниц сейчас перегружено всевозможной рекламой... Наша задача «вытащить»
из веб-страницы только полезную информацию, отбросив весь «мусор» (навигацию, рекламу и тд).

Полученный текст нужно отформатировать для максимально комфортного чтения в любом
текстовом редакторе. Правила форматирования: 
* ширина строки не больше 80 символов (еслибольше, переносим по словам), абзацы и заголовки отбиваются пустой строкой. 
* Если в тексте встречаются ссылки, то URL вставить в текст в квадратных скобках. Остальные правила на ваше
усмотрение.

Программа оформляется в виде утилиты командной строки, которой в качестве параметра
указывается произвольный URL. Она извлекает по этому URL страницу, обрабатывает ее и
формирует текстовый файл с текстом статьи, представленной на данной странице.

В качестве примера можно взять любую статью на lenta.ru, gazeta.ru и тд

Алгоритм должен быть максимально универсальным, то есть работать на большинстве сайтов.

__Таск 1*__: Имя выходного файла должно формироваться автоматически по URL.
Примерно так:
http://lenta.ru/news/2013/03/dtp/index.html => [CUR_DIR]/lenta.ru/news/2013/03/dtp/index.txt

__Таск 2*__: Программа должна поддаваться настройке – в отдельном файле/файлах
задаются шаблоны обработки страниц.

## Требования к выполнению задачи
1. Задача выполняется на Python с использованием классов. Не должно использоваться
сторонних библиотек, впрямую решающих задачу.
2. Предпочтительная среда выполнения – MS Windows.
3. Решение должно состоять из документа, описывающего алгоритм, исходных кодов
программы, исполняемого модуля.
4. Приложите список URL, на которых вы проверяли свое решение. И результаты проверки.
5. Желательно указать направление дальнейшего улучшения/развития программы.

## Идея решения
Будем исходить из предположения, что весь контент статьи обернут в один html-tag
Скорее всего такими тэгами будут: div, article, section, main

Так как мы ищем тексты статей, то нам нужно найти тэг, в котором содержится 
наибольшее количество предложений. 
Предложения будем вычленять регулярным выражением.

## Сторонние библиотеки:
- beautifulsoup4 для работы с DOM
- lxml как парсер для bs4

## Алгоритм работы:
- загружаем страницу с помощью requests.get
- формируем по контенту страницы объект BeautifulSoup
- очищаем soup от ненужных тэгов (e.g. script, link, iframe, img)
- пусть у нас есть список content_tags, содержащий названия тэгов, 
в которых мы ожидаем увидеть контент. Обычно подразумеваются тэги 
div, main, article, section. 
Мы будем искать элемент с типом из content_tags, содержащий наибольшее 
количество предложений. Причем мы будем считать предложения в дочерних узлах, 
только если тип дочернего узла не из content_tags: 
    - ищем узел с типом из content_tags, не содержащий дочерних узлов с типом из 
    content_tags
    - считаем в этом узле количество предложений и удаляем этот узел из DOM
    - goto шаг 2 пока есть такие узлы

- содержимое узла с наибольшим количеством предложений форматируем 
согласно правилам (заменяем в DOM тэги на текстовые фрагменты)
- сохраняем файл

## Описание файлов:
- main.py - основной исполняемый файл
- config.py - конфиг файл
- article_scraper.py - скрапер. работает с DOM, в том числе ищет узел с 
наибольшим количеством предложений
- filesystem_worker.py - класс FileSystemWorker для работы с ФС 
(создание папок, вычленение имени файла из URL)
- format_rules.py - правила форматирования тэгов
- misc.py - скачивание статьи
- txtmaker.py - класс TxtMaker служит для получения текстового файла из html 
по правилам форматирования

## Фичи:
- сохранение файлов в разные папки (таск 1*)
- настройка user-agent
- настройка длины для враппинга
- возможность задать игнорируемые тэги 
- возможность задать тэги, в которых ожидается найти основной контент
- написаны правила для markdown (e.g. h1 -> #)

## Что можно добавить:
- сделать возможность сохранять статью в pdf. Это позволит делать более красивое
форматирование и сохранять картинки
- развить работу с markdown (e.g. тэг code, списки)
- настроить отрисовку таблиц ASCII символами
- улучшить эвристику поиска основного узла. Учитывать, например, пунктуацию или
использовать NLP.

__NB!__ Список статей, которые я пробовал парсить приведен в batch-скрипте runmany.cmd
Результаты парсинга в папке articles
